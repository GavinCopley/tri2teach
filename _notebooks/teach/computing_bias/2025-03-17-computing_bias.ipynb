{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "layout: post\n",
    "categories: [CSP Sprint Objectives]\n",
    "title: Computing Bias\n",
    "description:  Computing Bias from Collegeboard's AP CSP curriculum\n",
    "type: issues \n",
    "courses: { csp: {week: 9} }\n",
    "comments: true\n",
    "permalink: /csp/teach/bias\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Computer Bias\n",
    "\n",
    "**Published on:** March 17, 2025  \n",
    "**Reading Time:** ~5 minutes  \n",
    "\n",
    "Have you ever noticed how Netflix keeps recommending the same types of shows over and over? Or how voice assistants (like Siri or Alexa) almost always have a female voice by default? These little quirks might seem harmless, but they can hint at something larger behind the scenes: **computer bias**.\n",
    "\n",
    "In this blog, we‚Äôll unpack what computer bias is, share some simple real-world examples, and explore how we can create more fair and inclusive technologies.\n",
    "\n",
    "---\n",
    "\n",
    "## What Is Computer Bias?\n",
    "\n",
    "Think of ‚Äúcomputer bias‚Äù as **unfair preferences** built into a computer system. Often, this bias isn‚Äôt deliberate‚Äîrather, it‚Äôs a result of how people **design, test, or use** these systems. Some typical ways bias sneaks in:\n",
    "\n",
    "1. **Data Issues**: If you train an algorithm on data that‚Äôs missing certain types of people or situations, the system might perform poorly for those it hasn‚Äôt ‚Äúseen‚Äù enough of.\n",
    "2. **Design Choices**: Maybe the developer assumed everyone uses the product the same way, leading to one-size-fits-all features that accidentally exclude some users.\n",
    "3. **Testing Gaps**: Sometimes, teams test only with certain groups‚Äîsay, colleagues or friends‚Äîmissing feedback from the broader population.\n",
    "\n",
    "> **Key Takeaway**: Humans are behind every tech tool, so our biases can show up in the algorithms we create‚Äîeven if we don‚Äôt mean for it to happen.\n",
    "\n",
    "---\n",
    "\n",
    "## Everyday Examples of Bias\n",
    "\n",
    "### 1. Netflix Recommendations\n",
    "Netflix is known for suggesting shows and movies based on what you‚Äôve watched before. While it‚Äôs helpful, the recommendation system can unintentionally ‚Äúpigeonhole‚Äù you. If you mostly watch comedies, Netflix might stop suggesting documentaries or foreign films, even if you‚Äôd actually love them. This can keep you stuck in a loop of the same types of content.\n",
    "\n",
    "**Why is that biased?**  \n",
    "- The algorithm heavily leans on past choices and might **ignore** other genres or shows that don‚Äôt fit its pattern.  \n",
    "- People who share a profile (like families) might get skewed recommendations that don‚Äôt reflect everyone‚Äôs interests.\n",
    "\n",
    "### 2. Virtual Assistants with Female Voices\n",
    "It‚Äôs common for digital assistants (like Siri or Alexa) to default to a female voice. While some platforms offer alternatives, the default often remains female.\n",
    "\n",
    "**Why might this be a problem?**  \n",
    "- It can subtly reinforce stereotypes that **women** are ‚Äúhelpers‚Äù or ‚Äúassistants.‚Äù  \n",
    "- It may exclude people who‚Äôd prefer a different voice or feel more comfortable with another default option.\n",
    "\n",
    "### 3. Social Media Age Gaps\n",
    "If you look at who uses TikTok (generally younger folks) versus who prefers Facebook (often older demographics), you‚Äôll see a clear age divide. Sometimes these platforms don‚Äôt explicitly **stop** people from different age groups from joining; however, their design, marketing, and trends can unintentionally favor one demographic over another.\n",
    "\n",
    "---\n",
    "\n",
    "## The HP Camera Incident: A Closer Look\n",
    "\n",
    "One famous example involved an HP laptop camera that couldn‚Äôt reliably track the faces of people with darker skin tones. A user posted a video calling the camera ‚Äúracist‚Äù because it followed lighter-skinned faces with ease but struggled to track darker-skinned faces.\n",
    "\n",
    "1. **Was it intentional?**  \n",
    "   - The user didn‚Äôt think HP deliberately designed it this way, but the final result was still unfair.\n",
    "\n",
    "2. **Why did it happen?**  \n",
    "   - Likely **limited test data** during development. If you only test facial tracking on people with lighter skin tones, you miss potential issues for everyone else.\n",
    "\n",
    "3. **Is it harmful?**  \n",
    "   - Yes. Beyond frustration, it alienates users and sends the message that the technology ‚Äúisn‚Äôt made‚Äù for them.\n",
    "\n",
    "4. **Should it be corrected?**  \n",
    "   - Absolutely. More comprehensive testing and more diverse datasets would make the camera work better for everyone.\n",
    "\n",
    "---\n",
    "\n",
    "## Avoiding Bias in Tech\n",
    "\n",
    "So, how do we stop bias from creeping into our algorithms and products? Here are a few **practical tips**:\n",
    "\n",
    "1. **Expand Your Data**  \n",
    "   - Gather as many different types of samples as possible. For Netflix-like recommendations, that might mean training on a wide range of viewing histories from diverse users.\n",
    "\n",
    "2. **Encourage Diverse Teams**  \n",
    "   - People from varied backgrounds ask different questions and notice different problems. This helps catch unintentional biases **before** a product launches.\n",
    "\n",
    "3. **Test, Test, Test**  \n",
    "   - Don‚Äôt just rely on your friend group or your coworkers. Try ‚Äúbeta testing‚Äù with users of all ages, races, and abilities. Seek feedback and see if certain groups are having more issues.\n",
    "\n",
    "4. **Document Your Assumptions**  \n",
    "   - Be transparent about how the algorithm makes decisions. If you know it‚Äôs heavily focused on past user behavior, note that clearly.\n",
    "\n",
    "---\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/uV3sWo0J4Pw\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
    "\n",
    "\n",
    "## Why It All Matters\n",
    "\n",
    "When biases go unchecked, technology can exclude people, reinforce negative stereotypes, or limit choices. By staying aware of potential pitfalls‚Äîfrom the data we collect to how we test and design our products‚Äîwe can build a more **inclusive** digital world. After all, technology should be for everyone.\n",
    "\n",
    "**Ready to take action?**  \n",
    "- **Try looking at your streaming platform‚Äôs recommendations.** Do they all look the same? If so, shake things up: watch a documentary or a foreign-language show to encourage variety in your recommendations.  \n",
    "- **Voice your preferences.** If you have a smart speaker or assistant, see if you can change the default voice. Notice how that small shift might affect how you interact with it.  \n",
    "- **Share your experiences.** If you see tech that doesn‚Äôt work well for certain groups, point it out. Often, engineers and designers aren‚Äôt aware there‚Äôs a problem until users speak up.\n",
    "\n",
    "---\n",
    "\n",
    "## Join the Conversation\n",
    "\n",
    "- **Have an example of bias you‚Äôve encountered in apps or websites?**  \n",
    "  Add a comment below! \n",
    "- **Working on your own project?**  \n",
    "  Test it with friends and classmates who might use it differently. You could catch issues early and create something that **everyone** can enjoy.\n",
    "\n",
    "We all have a part to play in recognizing and fighting computer bias. By being informed and proactive, we can help tech become a force that truly **includes** rather than excludes.\n",
    "\n",
    "üé¨Popcorn Hack #1: \n",
    "Most people stick with the default voice of Siri, Alexa, or Google Assistant. But did you know you can change it? Explore the settings to switch to a different accent or gender. This change can make it feel more personalized and challenge the built in biases.\n",
    "\n",
    "Popcorn Hack#2: \n",
    "What is computer bias, and how can it be caused by intentional or unintentional factors in software development? Provide an example and explain how programmers can reduce bias in their algorithms.\n",
    "\n",
    "Homework Hack#1:\n",
    "Imagine a university is using an AI system to screen scholarship applications. Over time, students notice that the system tends to select more applicants from wealthy neighborhoods while rejecting many from lower-income areas.\n",
    "   - What might be causing this bias?\n",
    "   - How could the university fix this issue to ensure fair selection for all students?\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
